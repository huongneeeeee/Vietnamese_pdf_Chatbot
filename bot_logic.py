from RAG_chatbot import rag_bot 
# Import tá»« pdf_processor (Ä‘Ãºng theo tÃªn file báº¡n Ä‘ang dÃ¹ng)
from pdf_processor import ContextRetriever 
from text_processor import TextProcessor
from operator import itemgetter

retriever = ContextRetriever("original_text")
text_processor = TextProcessor()

class chatBotMode:
    def __init__(self, vector_dbs: dict = None):
        self.vector_dbs = vector_dbs if vector_dbs is not None else {}

    def process_question(self, user_question: str, selected_pdfs: list = None, chat_history_str: str = ""):
        
        # --- BÆ¯á»šC 1: Lá»ŒC VECTOR DB ---
        if not selected_pdfs:
            target_dbs = self.vector_dbs
        else:
            target_dbs = {name: db for name, db in self.vector_dbs.items() 
                          if name in selected_pdfs}

        if not target_dbs:
            # [FIX] ThÃªm "context": "" Ä‘á»ƒ trÃ¡nh KeyError
            return {"response": "Vui lÃ²ng táº£i lÃªn hoáº·c chá»n Ã­t nháº¥t má»™t tÃ i liá»‡u Ä‘á»ƒ truy váº¥n.", "sources": [], "context": ""}

        # --- BÆ¯á»šC 2: Tá»I Æ¯U CÃ‚U Há»I (QUAN TRá»ŒNG CHO CÃ‚U Há»I NGáº®N) ---
        search_query = user_question
        
        # Náº¿u cÃ¢u há»i quÃ¡ ngáº¯n (dÆ°á»›i 20 kÃ½ tá»±) vÃ  chá»©a tá»« khÃ³a tÃ³m táº¯t
        # Ta thay tháº¿ báº±ng cÃ¢u query Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tÃ¬m kiáº¿m hiá»‡u quáº£ hÆ¡n
        if len(user_question.strip()) < 20:
            keywords = ["tÃ³m táº¯t", "summary", "chÃ­nh", "ná»™i dung", "Ã½ chÃ­nh", "overview"]
            if any(k in user_question.lower() for k in keywords):
                search_query = "Tá»•ng há»£p ná»™i dung chÃ­nh, cÃ¡c Ã½ quan trá»ng nháº¥t vÃ  káº¿t luáº­n cá»§a tÃ i liá»‡u."
                print(f"ğŸ”„ ÄÃ£ tá»‘i Æ°u cÃ¢u há»i ngáº¯n: '{user_question}' -> '{search_query}'")
        
        print(f"ğŸ” TÃ¬m kiáº¿m vá»›i tá»« khÃ³a: '{search_query}'")

        # --- BÆ¯á»šC 3: TÃŒM KIáº¾M (RETRIEVAL) ---
        results = []
        # TÄƒng ngÆ°á»¡ng tÃ¬m kiáº¿m lÃªn má»™t chÃºt Ä‘á»ƒ cháº¥p nháº­n nhiá»u thÃ´ng tin hÆ¡n cho viá»‡c tá»•ng há»£p
        SIMILARITY_THRESHOLD = 1.8  

        for db_name, db in target_dbs.items():
            # TÄƒng k lÃªn 6 Ä‘á»ƒ láº¥y nhiá»u Ä‘oáº¡n vÄƒn hÆ¡n tá»« nhiá»u file (phá»¥c vá»¥ tá»•ng há»£p)
            docs_scores = db.similarity_search_with_score(search_query, k=6)
            for doc, score in docs_scores:
                if score < SIMILARITY_THRESHOLD: 
                    results.extend([(doc, score, db_name)])

        if not results:
             # Fallback: Náº¿u khÃ´ng tÃ¬m tháº¥y gÃ¬ nhÆ°ng ngÆ°á»i dÃ¹ng muá»‘n tÃ³m táº¯t, 
             # thá»­ láº¥y trang Ä‘áº§u tiÃªn cá»§a file Ä‘áº§u tiÃªn lÃ m context (thÆ°á»ng lÃ  giá»›i thiá»‡u)
             if "tÃ³m táº¯t" in search_query.lower() or "ná»™i dung" in search_query.lower():
                 first_db_name = next(iter(target_dbs))
                 first_db = target_dbs[first_db_name]
                 # TÃ¬m kiáº¿m rá»™ng hÆ¡n
                 fallback_docs = first_db.similarity_search_with_score("giá»›i thiá»‡u", k=3)
                 results.extend([(doc, score, first_db_name) for doc, score in fallback_docs])
             
             if not results:
                return {"response": "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ o Ä‘á»§ liÃªn quan trong file báº¡n upload Ä‘á»ƒ tráº£ lá»i.", "sources": [], "context": ""}

        # Láº¥y top 6 Ä‘oáº¡n tá»‘t nháº¥t (Ä‘Ã£ tÄƒng tá»« 3 lÃªn 6) Ä‘á»ƒ AI cÃ³ Ä‘á»§ dá»¯ liá»‡u tá»•ng há»£p
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm sá»‘ (score cÃ ng tháº¥p cÃ ng giá»‘ng)
        top_docs = sorted(results, key=itemgetter(1))[:6]

        metadatas = []
        expanded_contexts = []

        for doc, score, db_name in top_docs:
            file_name = retriever.get_file_name(doc.metadata)
            doc.metadata['source_db'] = db_name
            metadatas.append(doc.metadata) 

            file_name_remove_accents = text_processor.remove_accents(file_name)
            expanded_context = retriever.expand_context(file_name_remove_accents, doc.page_content)
            
            # ThÃªm tÃªn file vÃ o context Ä‘á»ƒ AI biáº¿t thÃ´ng tin nÃ y Ä‘áº¿n tá»« Ä‘Ã¢u -> GiÃºp tá»•ng há»£p tá»‘t hÆ¡n
            context_with_source = f"[ThÃ´ng tin trÃ­ch tá»« file: {db_name}]:\n{expanded_context}"
            expanded_contexts.append(context_with_source)

        context_str = "\n\n".join(expanded_contexts)

        # --- BÆ¯á»šC 4: Gá»ŒI AI ---
        response_text = rag_bot.response(
            user_question=user_question, # Gá»­i cÃ¢u há»i gá»‘c cá»§a ngÆ°á»i dÃ¹ng
            chat_history=chat_history_str,
            context_data=context_str
        ).strip()

        # --- BÆ¯á»šC 5: Xá»¬ LÃ NGUá»’N ---
        sources_list = []
        for meta in metadatas:
            file_name = meta.get('source_db', 'Unknown File')
            page_number = meta.get('page') 
            if page_number is not None:
                sources_list.append(f"{file_name} (Trang {int(page_number) + 1})")
            else:
                sources_list.append(f"{file_name}")
        
        unique_sources = list(set(sources_list))
        
        return {
            "response": response_text,
            "context": context_str,
            "sources": unique_sources
        }